
@book{lablee_spectral_2015,
	location = {Zürich},
	title = {Spectral theory in Riemannian geometry},
	isbn = {978-3-03719-151-4},
	series = {{EMS} Textbooks in mathematics},
	publisher = {European mathematical society},
	author = {Lablée, Olivier},
	year = {2015},
	langid = {english},
}

@article{sarti_symplectic_2008,
	title = {The symplectic structure of the primary visual cortex},
	volume = {98},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-007-0194-9},
	doi = {10.1007/s00422-007-0194-9},
	pages = {33--48},
	number = {1},
	journal = {Biol Cybern},
	author = {Sarti, Alessandro and Citti, Giovanna and Petitot, Jean},
	urldate = {2021-07-15},
	year = {2008},
	langid = {english},
}


@article{kac_can_1966,
	title = {Can One Hear the Shape of a Drum?},
	volume = {73},
	issn = {00029890},
	url = {https://www.jstor.org/stable/2313748?origin=crossref},
	doi = {10.2307/2313748},
	pages = {1},
	number = {4},
	journal = {The American Mathematical Monthly},
	author = {Kac, Mark},
	urldate = {2021-08-05},
	year = {1966},
	langid = {english},
}

@book{citti_neuromathematics_2014,
	location = {Berlin, Heidelberg},
	title = {Neuromathematics of Vision},
	isbn = {978-3-642-34443-5 978-3-642-34444-2},
	url = {http://link.springer.com/10.1007/978-3-642-34444-2},
	series = {Lecture Notes in Morphogenesis},
	publisher = {Springer Berlin Heidelberg},
	author = {Citti, Giovanna and Sarti, Alessandro},
	urldate = {2021-09-24},
	year = {2014},
	langid = {english},
	doi = {10.1007/978-3-642-34444-2},
}



@article{kolda_tensor_2009,
	title = {Tensor Decompositions and Applications},
	volume = {51},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/07070111X},
	doi = {10.1137/07070111X},
	pages = {455--500},
	number = {3},
	journal = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	urldate = {2021-06-10},
	year = {2009},
	langid = {english},
	file = {Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\UVD8WLB2\\Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf},
}
@article{pak_random_1997,
    author = {Pak, Igor},
	title = {Random WALKS ON GROUPS: STRONG UNIFORM TIME APPROACH},
	year = {1997},
}


@article{chung_neural_2021,
	title = {Neural population geometry: An approach for understanding biological and artificial neural networks},
	pages = {12},
	year = {2021},
	author = {Chung, {SueYeon} and Abbott, L F},
	langid = {english},
}


@article{ceccherini-silberstein_harmonic_2008,
	title = {Harmonic Analysis on Finite Groups: Representation Theory, Gelfand Pairs and Markov Chains},
	pages = {456},
	author = {Ceccherini-Silberstein, Tullio and Scarabotti, Fabio and Tolli, Filippo},
	langid = {english},
	year = {2008},
}

@article{diaconis_group_1988,
	title = {Group Representations in Probability and Statistics},
	pages = {206},
	author = {Diaconis, Persi},
	langid = {english},
	year = {1988},
}


@article{chung_classification_2018,
	title = {Classification and Geometry of General Perceptual Manifolds},
	volume = {8},
	issn = {2160-3308},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031003},
	doi = {10.1103/PhysRevX.8.031003},
	pages = {031003},
	number = {3},
	journal = {Phys. Rev. X},
	author = {Chung, {SueYeon} and Lee, Daniel D. and Sompolinsky, Haim},
	urldate = {2021-10-23},
	year = {2018},
	langid = {english},
}

@article{Lindsay2021,
   title={Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},
   volume={33},
   ISSN={1530-8898},
   url={http://dx.doi.org/10.1162/jocn_a_01544},
   DOI={10.1162/jocn_a_01544},
   number={10},
   journal={Journal of Cognitive Neuroscience},
   publisher={MIT Press - Journals},
   author={Lindsay, Grace W.},
   year={2021},
   month={Sep},
   pages={2017–2031}
}
@MISC{pca-se,
    TITLE = {Making sense of principal component analysis, eigenvectors &amp; eigenvalues},AUTHOR = {amoeba (https://stats.stackexchange.com/users/28666/amoeba)},HOWPUBLISHED = {Cross Validated},NOTE = {URL:https://stats.stackexchange.com/q/140579 (version: 2021-06-24)},EPRINT = {https://stats.stackexchange.com/q/140579}, URL = {https://stats.stackexchange.com/q/140579}
}
@article{Kol2009,
author = {Kolda, Tamara G. and Bader, Brett W.},
title = {Tensor Decompositions and Applications},
year = {2009},
issue_date = {August 2009},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {51},
number = {3},
issn = {0036-1445},
url = {https://doi.org/10.1137/07070111X},
doi = {10.1137/07070111X},
journal = {SIAM Rev.},
month = aug,
pages = {455–500},
numpages = {46},
keywords = {tensor decompositions, multilinear algebra, higher-order singular value decomposition (HOSVD), parallel factors (PARAFAC), multiway arrays, higher-order principal components analysis (Tucker), canonical decomposition (CANDECOMP)}
}

@article {Gwilliams221630,
	author = {Gwilliams, Laura and King, Jean-R{\'e}mi},
	title = {Performance-optimized hierarchical models only partially predict neural responses during perceptual decision making},
	elocation-id = {221630},
	year = {2017},
	doi = {10.1101/221630},
	publisher = {Cold Spring Harbor Laboratory},
	journal = {bioRxiv}
}
@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
}


@article{fefferman_testing_2016,
	title = {Testing the manifold hypothesis},
	volume = {29},
	issn = {0894-0347, 1088-6834},
	url = {https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/},
	doi = {10.1090/jams/852},
	pages = {983--1049},
	number = {4},
	journal = {J. Amer. Math. Soc.},
	author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
	urldate = {2021-10-25},
	year = {2016},
	langid = {english},
}

 @misc{deepai_2019, title={Manifold hypothesis}, url={https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis}, journal={DeepAI}, publisher={DeepAI}, author={DeepAI}, year={2019}}
 
 
@article{gallego_neural_2017,
	title = {Neural Manifolds for the Control of Movement},
	volume = {94},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317304634},
	doi = {10.1016/j.neuron.2017.05.025},
	pages = {978--984},
	number = {5},
	journal = {Neuron},
	author = {Gallego, Juan A. and Perich, Matthew G. and Miller, Lee E. and Solla, Sara A.},
	urldate = {2021-10-23},
	year = {2017},
	langid = {english},
}


@article{fukushima_neocognitron_1980,
	title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	shorttitle = {Neocognitron},
	pages = {193--202},
	number = {4},
	journal = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	urldate = {2021-10-25},
	year = {1980},
	langid = {english},
}

@article{williams_unsupervised_2018,
	title = {Unsupervised Discovery of Demixed, Low-Dimensional Neural Dynamics across Multiple Timescales through Tensor Component Analysis},
	volume = {98},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318303878},
	doi = {10.1016/j.neuron.2018.05.015},
	pages = {1099--1115.e8},
	number = {6},
	journal = {Neuron},
	author = {Williams, Alex H. and Kim, Tony Hyun and Wang, Forea and Vyas, Saurabh and Ryu, Stephen I. and Shenoy, Krishna V. and Schnitzer, Mark and Kolda, Tamara G. and Ganguli, Surya},
	urldate = {2021-06-10},
	year = {2018},
	langid = {english},
	file = {Williams et al. - 2018 - Unsupervised Discovery of Demixed, Low-Dimensional.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\EZ4DD99Y\\Williams et al. - 2018 - Unsupervised Discovery of Demixed, Low-Dimensional.pdf:application/pdf},
}


@article{nieh_geometry_2021,
	title = {Geometry of abstract learned knowledge in the hippocampus},
	volume = {595},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-021-03652-7},
	doi = {10.1038/s41586-021-03652-7},
	pages = {80--84},
	number = {7865},
	journal = {Nature},
	author = {Nieh, Edward H. and Schottdorf, Manuel and Freeman, Nicolas W. and Low, Ryan J. and Lewallen, Sam and Koay, Sue Ann and Pinto, Lucas and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W.},
	urldate = {2021-10-25},
	year = {2021},
	langid = {english},
	file = {Nieh et al. - 2021 - Geometry of abstract learned knowledge in the hipp.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\XIT3S39F\\Nieh et al. - 2021 - Geometry of abstract learned knowledge in the hipp.pdf:application/pdf},
}

@article{hubel_receptive_1962,
	title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
	volume = {160},
	issn = {0022-3751},
	url = {https://pubmed.ncbi.nlm.nih.gov/14449617},
	doi = {10.1113/jphysiol.1962.sp006837},
	language = {eng},
	number = {1},
	journal = {The Journal of physiology},
	author = {Hubel, D H and Wiesel, T N},
	month = jan,
	year = {1962},
	keywords = {*CEREBRAL CORTEX/physiology, *Visual Cortex, Animals, Cats, Cerebral Cortex/*physiology},
	pages = {106--154},
}

@article{yamashita_convolutional_2018,
	title = {Convolutional neural networks: an overview and application in radiology},
	volume = {9},
	issn = {1869-4101},
	url = {https://doi.org/10.1007/s13244-018-0639-9},
	doi = {10.1007/s13244-018-0639-9},
	number = {4},
	journal = {Insights into Imaging},
	author = {Yamashita, Rikiya and Nishio, Mizuho and Do, Richard Kinh Gian and Togashi, Kaori},
	month = aug,
	year = {2018},
	pages = {611--629},
}

@article{martinez_complex_2003,
	title = {Complex {Receptive} {Fields} in {Primary} {Visual} {Cortex}},
	volume = {9},
	issn = {1073-8584},
	url = {https://doi.org/10.1177/1073858403252732},
	doi = {10.1177/1073858403252732},
	number = {5},
	urldate = {2021-11-05},
	journal = {The Neuroscientist},
	author = {Martinez, Luis M. and Alonso, Jose-Manuel},
	month = oct,
	year = {2003},
	note = {Publisher: SAGE Publications Inc STM},
	pages = {317--331},
}
@article{poggio2003mathematics,
  title={The mathematics of learning: Dealing with data},
  author={Poggio, Tomaso and Smale, Steve and others},
  journal={Notices of the AMS},
  volume={50},
  number={5},
  pages={537--544},
  year={2003}
}

@inproceedings{geman1999hierarchy,
  title={Hierarchy in machine and natural vision},
  author={Geman, Stuart},
  booktitle={Proceedings of the Scandinavian Conference on Image Analysis},
  volume={1},
  pages={179--184},
  year={1999}
}

@article{riesenhuber1999hierarchical,
  title={Hierarchical models of object recognition in cortex},
  author={Riesenhuber, Maximilian and Poggio, Tomaso},
  journal={Nature neuroscience},
  volume={2},
  number={11},
  pages={1019--1025},
  year={1999},
  publisher={Nature Publishing Group}
}
@article{dicarlo2012does,
  title={How does the brain solve visual object recognition?},
  author={DiCarlo, James J and Zoccolan, Davide and Rust, Nicole C},
  journal={Neuron},
  volume={73},
  number={3},
  pages={415--434},
  year={2012},
  publisher={Elsevier}
}

@article{hegde2007reappraising,
  title={Reappraising the functional implications of the primate visual anatomical hierarchy},
  author={Hegde, Jay and Felleman, Daniel J},
  journal={The Neuroscientist},
  volume={13},
  number={5},
  pages={416--421},
  year={2007},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{hoffmann1972relay,
  title={Relay of receptive-field properties in dorsal lateral geniculate nucleus of the cat.},
  author={Hoffmann, KP and Stone, J and Sherman, S Murray},
  journal={Journal of Neurophysiology},
  volume={35},
  number={4},
  pages={518--531},
  year={1972}
}


@article{giusti_clique_2015,
	title = {Clique topology reveals intrinsic geometric structure in neural correlations},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1506407112},
	doi = {10.1073/pnas.1506407112},
	abstract = {Detecting meaningful structure in neural activity and connectivity data is challenging in the presence of hidden nonlinearities, where traditional eigenvalue-based methods may be misleading. We introduce a novel approach to matrix analysis, called clique topology, that extracts features of the data invariant under nonlinear monotone transformations. These features can be used to detect both random and geometric structure, and depend only on the relative ordering of matrix entries. We then analyzed the activity of pyramidal neurons in rat hippocampus, recorded while the animal was exploring a 2D environment, and confirmed that our method is able to detect geometric organization using only the intrinsic pattern of neural correlations. Remarkably, we found similar results during nonspatial behaviors such as wheel running and rapid eye movement (REM) sleep. This suggests that the geometric structure of correlations is shaped by the underlying hippocampal circuits and is not merely a consequence of position coding. We propose that clique topology is a powerful new tool for matrix analysis in biological settings, where the relationship of observed quantities to more meaningful variables is often nonlinear and unknown.},
	language = {en},
	number = {44},
	urldate = {2021-11-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
	month = nov,
	year = {2015},
	pages = {13455--13460},
	file = {Giusti et al. - 2015 - Clique topology reveals intrinsic geometric struct.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\AQT45EYS\\Giusti et al. - 2015 - Clique topology reveals intrinsic geometric struct.pdf:application/pdf},
}

@article{bardin_topological_2019,
	title = {Topological exploration of artificial neuronal network dynamics},
	volume = {3},
	issn = {2472-1751},
	url = {https://direct.mit.edu/netn/article/3/3/725-743/2173},
	doi = {10.1162/netn_a_00080},
	abstract = {One of the paramount challenges in neuroscience is to understand the dynamics of individual neurons and how they give rise to network dynamics when interconnected. Historically, researchers have resorted to graph theory, statistics, and statistical mechanics to describe the spatiotemporal structure of such network dynamics. Our novel approach employs tools from algebraic topology to characterize the global properties of network structure and dynamics.},
	language = {en},
	number = {3},
	urldate = {2021-11-05},
	journal = {Network Neuroscience},
	author = {Bardin, Jean-Baptiste and Spreemann, Gard and Hess, Kathryn},
	month = jan,
	year = {2019},
	pages = {725--743},
	file = {Bardin et al. - 2019 - Topological exploration of artificial neuronal net.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\I9LLUNB2\\Bardin et al. - 2019 - Topological exploration of artificial neuronal net.pdf:application/pdf},
}

@techreport{beshkov_geodesic-based_2021,
	type = {preprint},
	title = {Geodesic-based distance reveals non-linear topological features in neural activity from mouse visual cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.05.21.444993},
	abstract = {An increasingly popular approach to the analysis of neural data is to treat activity patterns as being constrained to and sampled from a manifold, which can be characterized by its topology. The persistent homology method identiﬁes the type and number of holes in the manifold thereby yielding functional information about the coding and dynamic properties of the underlying neural network. In this work we give examples of highly non-linear manifolds in which the persistent homology algorithm fails when it uses the Euclidean distance which does not always yield a good approximation of the true distance distribution of a point cloud sampled from a manifold. To deal with this issue we propose a simple strategy for the estimation of the geodesic distance which is a better approximation of the true distance distribution and can be used to successfully identify highly non-linear features with persistent homology. To document the utility of our method we model a circular manifold, based on orthogonal sinusoidal basis functions and compare how the chosen metric determines the performance of the persistent homology algorithm. Furthermore we discuss the robustness of our method across diﬀerent manifold properties and point out strategies for interpreting its results as well as some possible pitfalls of its application. Finally we apply this analysis to neural data coming from the Visual Coding - Neuropixels dataset recorded in mouse visual cortex after stimulation with drifting gratings at the Allen Institute. We ﬁnd that diﬀerent manifolds with a non-trivial topology can be seen across regions and stimulus properties. Finally, we discuss what these manifolds say about visual computation and how they depend on stimulus parameters.},
	language = {en},
	urldate = {2021-11-05},
	institution = {Neuroscience},
	author = {Beshkov, Kosio and Tiesinga, Paul},
	month = may,
	year = {2021},
	doi = {10.1101/2021.05.21.444993},
	file = {Beshkov and Tiesinga - 2021 - Geodesic-based distance reveals non-linear topolog.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\YDUHDVQE\\Beshkov and Tiesinga - 2021 - Geodesic-based distance reveals non-linear topolog.pdf:application/pdf},
}

@article{singh_topological_2008,
	title = {Topological analysis of population activity in visual cortex},
	volume = {8},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/8.8.11},
	doi = {10.1167/8.8.11},
	abstract = {Information in the cortex is thought to be represented by the joint activity of neurons. Here we describe how fundamental questions about neural representation can be cast in terms of the topological structure of population activity. A new method, based on the concept of persistent homology, is introduced and applied to the study of population activity in primary visual cortex (V1). We found that the topological structure of activity patterns when the cortex is spontaneously active is similar to those evoked by natural image stimulation and consistent with the topology of a two sphere. We discuss how this structure could emerge from the functional organization of orientation and spatial frequency maps and their mutual relationship. Our ﬁndings extend prior results on the relationship between spontaneous and evoked activity in V1 and illustrates how computational topology can help tackle elementary questions about the representation of information in the nervous system.},
	language = {en},
	number = {8},
	urldate = {2021-11-05},
	journal = {Journal of Vision},
	author = {Singh, G. and Memoli, F. and Ishkhanov, T. and Sapiro, G. and Carlsson, G. and Ringach, D. L.},
	month = jun,
	year = {2008},
	pages = {11--11},
	file = {Singh et al. - 2008 - Topological analysis of population activity in vis.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\UDSMWDS2\\Singh et al. - 2008 - Topological analysis of population activity in vis.pdf:application/pdf},
}

@phdthesis{dyballa_manifold_2021,
	address = {Ann Arbor},
	type = {Ph.{D}.},
	title = {The {Manifold} of {Neural} {Responses} {Informs} {Physiological} {Circuits} in the {Visual} {System}},
	url = {https://www.proquest.com/dissertations-theses/manifold-neural-responses-informs-physiological/docview/2557257234/se-2?accountid=10267},
	abstract = {The rapid development of multi-electrode and imaging techniques is leading to a data explosion in neuroscience, opening the possibility of truly understanding the organization and functionality of our visual systems. Furthermore, the need for more natural visual stimuli greatly increases the complexity of the data. Together, these create a challenge for machine learning. Our goal in this thesis is to develop one such technique. The central pillar of our contribution is designing a manifold of neurons, and providing an algorithmic approach to inferring it. This manifold is functional, in the sense that nearby neurons on the manifold respond similarly (in time) to similar aspects of the stimulus ensemble. By organizing the neurons, our manifold differs from other, standard manifolds as they are used in visual neuroscience which instead organize the stimuli.Our contributions to the machine learning component of the thesis are twofold. First, we develop a tensor representation of the data, adopting a multilinear view of potential circuitry. Tensor factorization then provides an intermediate representation between the neural data and the manifold. We found that the rank of the neural factor matrix can be used to select an appropriate number of tensor factors. Second, to apply manifold learning techniques, a similarity kernel on the data must be defined. Like many others, we employ a Gaussian kernel, but refine it based on a proposed graph sparsification technique—this makes the resulting manifolds less sensitive to the choice of bandwidth parameter.We apply this method to neuroscience data recorded from retina and primary visual cortex in the mouse. For the algorithm to work, however, the underlying circuitry must be exercised to as full an extent as possible. To this end, we develop an ensemble of flow stimuli, which simulate what the mouse would 'see' running through a field. Applying the algorithm to the retina reveals that neurons form clusters corresponding to known retinal ganglion cell types. In the cortex, a continuous manifold is found, indicating that, from a functional circuit point of view, there may be a continuum of cortical function types. Interestingly, both manifolds share similar global coordinates, which hint at what the key ingredients to vision might be.Lastly, we turn to perhaps the most widely used model for the cortex: deep convolutional networks. Their feedforward architecture leads to manifolds that are even more clustered than the retina, and not at all like that of the cortex. This suggests, perhaps, that they may not suffice as general models for Artificial Intelligence.},
	language = {English},
	school = {Yale University},
	author = {Dyballa, Luciano},
	year = {2021},
	note = {ISBN: 9798534651942
Publication Title: ProQuest Dissertations and Theses
28321876},
	keywords = {0317:Neurosciences, 0719:Physiology, 0984:Computer science, Algorithms, Brain, Computer science, Datasets, Experiments, Gaussian kernel, Graph sparsification technique, Machine learning, Maps, Neighborhoods, Neural networks, Neurons, Neurosciences, Physiology, Retina, Tensor factorization},
}

@Comment  {Please use the identifier format NnnYYYYa for single author papers and NMOPQYYYYa for multiauthor papers, where Nnn is the first 3 letters of the last name of the author, NMPQ are the first letters of the last names of each of the authors, in alphabetical order, YYYY is the 4 digit year of publication, a is "a" for the first paper, "b" for the second, etc, with the same characters before it. Make sure you search to see you are not adding the same work again or reusing an identifier.}

@article{,
  author = {},
  title = {},
  year = {},
  journal = {},
  volume = {},
  issue = {},
  pages = {}
}

@book{Hat2002a,
  author = {Hatcher, Allen},
  title = {Algebraic Topology},
  year = {2002},
  publisher = {Cambridge University Press}
}

@article{Car2009a,
  author = {Carlsson, Gunnar},
  title = {Topology and Data},
  year = {2009},
  journal = {BULLETIN (New Series) OF THE AMERICAN MATHEMATICAL SOCIETY},
  volume = {46},
  issue = {2},
  pages = {255-308}
}

@book{Zom2005a,
  author = {Zomorodian, Afra J.},
  title = {Topology for Computing},
  year = {2005},
  publisher = {Cambridge University Press}
}


@article{MD2015a,
  author = {Medina, Patrick S. and Doerge, R W. },
  title = {Statistical Methods in Topological Data Analysis for Complex, High-Dimensional Data},
  year = {2015},
  journal = {Conference on Applied Statistics in Agriculture}
}

@inproceedings{Sav2019a,
    title = {Topological Data Analysis for Discourse Semantics?},
    author = {Savle, Ketki and Zadrozny, Wlodek  and Lee, Minwoo},
    booktitle = {Proceedings of the 13th International Conference on Computational Semantics - Student Papers},
    month ={May},
    year = {2019},
    address = {Gothenburg, Sweden},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W19-0605},
    doi = {10.18653/v1/W19-0605},
    pages = {34--43}
}

@article{BS2011a,
  author = {Baez, John C. and Stay, Mike },
  title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
  year = {2011},
  journal = {New Structures for Physics, Lecture Notes in Physics, Springer-Verlag Berlin Heidelberg},
  volume = {813},
  pages = {95}
}


@book{lablee_spectral_2015,
	location = {Zürich},
	title = {Spectral theory in Riemannian geometry},
	isbn = {978-3-03719-151-4},
	series = {{EMS} Textbooks in mathematics},
	publisher = {European mathematical society},
	author = {Lablée, Olivier},
	year = {2015},
	langid = {english},
}

@article{sarti_symplectic_2008,
	title = {The symplectic structure of the primary visual cortex},
	volume = {98},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-007-0194-9},
	doi = {10.1007/s00422-007-0194-9},
	pages = {33--48},
	number = {1},
	journal = {Biol Cybern},
	author = {Sarti, Alessandro and Citti, Giovanna and Petitot, Jean},
	urldate = {2021-07-15},
	year = {2008},
	langid = {english},
}

@article{coifman_geometric_2005,
	title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	volume = {102},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0500334102},
	doi = {10.1073/pnas.0500334102},
	shorttitle = {Geometric diffusions as a tool for harmonic analysis and structure definition of data},
	pages = {7426--7431},
	number = {21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Coifman, R. R. and Lafon, S. and Lee, A. B. and Maggioni, M. and Nadler, B. and Warner, F. and Zucker, S. W.},
	urldate = {2021-08-05},
	year = {2005},
	langid = {english},
}

@article{coifman_diffusion_2006,
	title = {Diffusion maps},
	volume = {21},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
	doi = {10.1016/j.acha.2006.04.006},
	pages = {5--30},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, Stéphane},
	urldate = {2021-08-05},
	year = {2006},
	langid = {english},
}

@article{kac_can_1966,
	title = {Can One Hear the Shape of a Drum?},
	volume = {73},
	issn = {00029890},
	url = {https://www.jstor.org/stable/2313748?origin=crossref},
	doi = {10.2307/2313748},
	pages = {1},
	number = {4},
	journal = {The American Mathematical Monthly},
	author = {Kac, Mark},
	urldate = {2021-08-05},
	year = {1966},
	langid = {english},
}

@book{citti_neuromathematics_2014,
	location = {Berlin, Heidelberg},
	title = {Neuromathematics of Vision},
	isbn = {978-3-642-34443-5 978-3-642-34444-2},
	url = {http://link.springer.com/10.1007/978-3-642-34444-2},
	series = {Lecture Notes in Morphogenesis},
	publisher = {Springer Berlin Heidelberg},
	author = {Citti, Giovanna and Sarti, Alessandro},
	urldate = {2021-09-24},
	year = {2014},
	langid = {english},
	doi = {10.1007/978-3-642-34444-2},
}


@article{hong_generalized_2020,
	title = {Generalized Canonical Polyadic Tensor Decomposition},
	volume = {62},
	issn = {0036-1445, 1095-7200},
	url = {http://arxiv.org/abs/1808.07452},
	doi = {10.1137/18M1203626},
	pages = {133--163},
	number = {1},
	journal = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Hong, David and Kolda, Tamara G. and Duersch, Jed A.},
	urldate = {2021-06-10},
	year = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.07452},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Hong et al. - 2020 - Generalized Canonical Polyadic Tensor Decompositio.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\SNZKNYNP\\Hong et al. - 2020 - Generalized Canonical Polyadic Tensor Decompositio.pdf:application/pdf},
}

@article{pak_random_1997,
    author = {Pak, Igor},
	title = {Random WALKS ON GROUPS: STRONG UNIFORM TIME APPROACH},
	year = {1997},
}




@article{ceccherini-silberstein_harmonic_2008,
	title = {Harmonic Analysis on Finite Groups: Representation Theory, Gelfand Pairs and Markov Chains},
	pages = {456},
	author = {Ceccherini-Silberstein, Tullio and Scarabotti, Fabio and Tolli, Filippo},
	langid = {english},
	year = {2008},
}

@article{diaconis_group_1988,
	title = {Group Representations in Probability and Statistics},
	pages = {206},
	author = {Diaconis, Persi},
	langid = {english},
	year = {1988},
}


@article{chung_classification_2018,
	title = {Classification and Geometry of General Perceptual Manifolds},
	volume = {8},
	issn = {2160-3308},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031003},
	doi = {10.1103/PhysRevX.8.031003},
	pages = {031003},
	number = {3},
	journal = {Phys. Rev. X},
	author = {Chung, {SueYeon} and Lee, Daniel D. and Sompolinsky, Haim},
	urldate = {2021-10-23},
	year = {2018},
	langid = {english},
}

@article{Lindsay2021,
   title={Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},
   volume={33},
   ISSN={1530-8898},
   url={http://dx.doi.org/10.1162/jocn_a_01544},
   DOI={10.1162/jocn_a_01544},
   number={10},
   journal={Journal of Cognitive Neuroscience},
   publisher={MIT Press - Journals},
   author={Lindsay, Grace W.},
   year={2021},
   month={Sep},
   pages={2017–2031}
}
@MISC{pca-se,
    TITLE = {Making sense of principal component analysis, eigenvectors &amp; eigenvalues},AUTHOR = {amoeba (https://stats.stackexchange.com/users/28666/amoeba)},HOWPUBLISHED = {Cross Validated},NOTE = {URL:https://stats.stackexchange.com/q/140579 (version: 2021-06-24)},EPRINT = {https://stats.stackexchange.com/q/140579}, URL = {https://stats.stackexchange.com/q/140579}
}
@article{Kol2009,
author = {Kolda, Tamara G. and Bader, Brett W.},
title = {Tensor Decompositions and Applications},
year = {2009},
issue_date = {August 2009},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {51},
number = {3},
issn = {0036-1445},
url = {https://doi.org/10.1137/07070111X},
doi = {10.1137/07070111X},
journal = {SIAM Rev.},
month = aug,
pages = {455–500},
numpages = {46},
keywords = {tensor decompositions, multilinear algebra, higher-order singular value decomposition (HOSVD), parallel factors (PARAFAC), multiway arrays, higher-order principal components analysis (Tucker), canonical decomposition (CANDECOMP)}
}

@inproceedings{memoli_gromov-hausdorff_2008,
	address = {Anchorage, AK, USA},
	title = {Gromov-{Hausdorff} distances in {Euclidean} spaces},
	isbn = {978-1-4244-2339-2},
	url = {http://ieeexplore.ieee.org/document/4563074/},
	doi = {10.1109/CVPRW.2008.4563074},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {2008 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Memoli, Facundo},
	month = jun,
	year = {2008},
	pages = {1--8},
	file = {Memoli - 2008 - Gromov-Hausdorff distances in Euclidean spaces.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\SXTJB6LI\\Memoli - 2008 - Gromov-Hausdorff distances in Euclidean spaces.pdf:application/pdf},
}

@article{mileyko_probability_2011,
	title = {Probability measures on the space of persistence diagrams},
	volume = {27},
	issn = {0266-5611, 1361-6420},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/27/12/124007},
	doi = {10.1088/0266-5611/27/12/124007},
	language = {en},
	number = {12},
	urldate = {2021-09-21},
	journal = {Inverse Problems},
	author = {Mileyko, Yuriy and Mukherjee, Sayan and Harer, John},
	month = dec,
	year = {2011},
	pages = {124007},
	file = {Mileyko et al. - 2011 - Probability measures on the space of persistence d.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\8CI64CDW\\Mileyko et al. - 2011 - Probability measures on the space of persistence d.pdf:application/pdf},
}

@article{kerber_geometry_2016,
	title = {Geometry {Helps} to {Compare} {Persistence} {Diagrams}},
	url = {http://arxiv.org/abs/1606.03357},
	language = {en},
	urldate = {2021-10-01},
	journal = {arXiv:1606.03357 [cs]},
	author = {Kerber, Michael and Morozov, Dmitriy and Nigmetov, Arnur},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03357},
	keywords = {Computer Science - Computational Geometry, G.2.2, G.4},
	file = {Kerber et al. - 2016 - Geometry Helps to Compare Persistence Diagrams.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\KJFTNN3Q\\Kerber et al. - 2016 - Geometry Helps to Compare Persistence Diagrams.pdf:application/pdf},
}

@inproceedings{memoli_comparing_2004,
	address = {Nice, France},
	title = {Comparing point clouds},
	isbn = {978-3-905673-13-5},
	url = {http://portal.acm.org/citation.cfm?doid=1057432.1057436},
	doi = {10.1145/1057432.1057436},
	language = {en},
	urldate = {2021-10-01},
	booktitle = {Proceedings of the 2004 {Eurographics}/{ACM} {SIGGRAPH} symposium on {Geometry} processing  - {SGP} '04},
	publisher = {ACM Press},
	author = {Mémoli, Facundo and Sapiro, Guillermo},
	year = {2004},
	note = {ISSN: 17278384},
	pages = {32},
	file = {Mémoli and Sapiro - 2004 - Comparing point clouds.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\ZLDNVPTX\\Mémoli and Sapiro - 2004 - Comparing point clouds.pdf:application/pdf},
}

@article{chazal_optimal_2013,
	title = {Optimal rates of convergence for persistence diagrams in {Topological} {Data} {Analysis}},
	url = {http://arxiv.org/abs/1305.6239},
	language = {en},
	urldate = {2021-10-01},
	journal = {arXiv:1305.6239 [cs, math, stat]},
	author = {Chazal, Frédéric and Glisse, Marc and Labruère, Catherine and Michel, Bertrand},
	month = may,
	year = {2013},
	note = {arXiv: 1305.6239},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Geometry, Mathematics - Statistics Theory, Mathematics - Geometric Topology},
	file = {Chazal et al. - 2013 - Optimal rates of convergence for persistence diagr.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\9JQTZ4YU\\Chazal et al. - 2013 - Optimal rates of convergence for persistence diagr.pdf:application/pdf},
}

@article{carriere_sliced_nodate,
	title = {Sliced {Wasserstein} {Kernel} for {Persistence} {Diagrams}},
	language = {en},
	author = {Carrière, Mathieu and Cuturi, Marco and Oudot, Steve},
	pages = {10},
	file = {Carrière et al. - Sliced Wasserstein Kernel for Persistence Diagrams.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\5JPPZEJQ\\Carrière et al. - Sliced Wasserstein Kernel for Persistence Diagrams.pdf:application/pdf},
}

@incollection{goos_surprising_2001,
	address = {Berlin, Heidelberg},
	title = {On the {Surprising} {Behavior} of {Distance} {Metrics} in {High} {Dimensional} {Space}},
	volume = {1973},
	isbn = {978-3-540-41456-8 978-3-540-44503-6},
	url = {http://link.springer.com/10.1007/3-540-44503-X_27},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Database {Theory} — {ICDT} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Van den Bussche, Jan and Vianu, Victor},
	year = {2001},
	doi = {10.1007/3-540-44503-X_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {420--434},
	file = {Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf:C\:\\Users\\ifisa\\Zotero\\storage\\WA9CBRZX\\Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf:application/pdf},
}

@article{mullner_fastcluster_2013,
	title = {\textbf{fastcluster} : {Fast} {Hierarchical}, {Agglomerative} {Clustering} {Routines} for \textit{{R}} and \textit{{Python}}},
	volume = {53},
	issn = {1548-7660},
	shorttitle = {\textbf{fastcluster}},
	url = {http://www.jstatsoft.org/v53/i09/},
	doi = {10.18637/jss.v053.i09},
	abstract = {The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most eﬃcient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.},
	language = {en},
	number = {9},
	urldate = {2021-09-21},
	journal = {Journal of Statistical Software},
	author = {Müllner, Daniel},
	year = {2013},
	file = {Müllner - 2013 - fastcluster  Fast Hierarchical, Agglomerat.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\6I626UCN\\Müllner - 2013 - fastcluster  Fast Hierarchical, Agglomerat.pdf:application/pdf},
}

@article{chazal_introduction_2021,
	title = {An introduction to {Topological} {Data} {Analysis}: fundamental and practical aspects for data scientists},
	shorttitle = {An introduction to {Topological} {Data} {Analysis}},
	url = {http://arxiv.org/abs/1710.04019},
	abstract = {Topological Data Analysis (tda) is a recent and fast growing ﬁeld providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of tda for non experts.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1710.04019 [cs, math, stat]},
	author = {Chazal, Frédéric and Michel, Bertrand},
	month = feb,
	year = {2021},
	note = {arXiv: 1710.04019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory, Mathematics - Algebraic Topology},
	file = {Chazal and Michel - 2021 - An introduction to Topological Data Analysis fund.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\UGHIH6FQ\\Chazal and Michel - 2021 - An introduction to Topological Data Analysis fund.pdf:application/pdf},
}

@article{singh_topological_nodate,
	title = {Topological {Methods} for the {Analysis} of {High} {Dimensional} {Data} {Sets} and {3D} {Object} {Recognition}},
	abstract = {We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions deﬁned on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.},
	language = {en},
	author = {Singh, Gurjeet and Mémoli, Facundo and Carlsson, Gunnar},
	pages = {11},
	file = {Singh et al. - Topological Methods for the Analysis of High Dimen.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\W5JINNN3\\Singh et al. - Topological Methods for the Analysis of High Dimen.pdf:application/pdf},
}

@article{carlsson_topology_2009,
	title = {Topology and data},
	volume = {46},
	issn = {0273-0979},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-09-01249-X},
	doi = {10.1090/S0273-0979-09-01249-X},
	language = {en},
	number = {2},
	urldate = {2021-11-02},
	journal = {Bulletin of the American Mathematical Society},
	author = {Carlsson, Gunnar},
	month = jan,
	year = {2009},
	pages = {255--308},
	file = {Carlsson - 2009 - Topology and data.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\K4HSI5TC\\Carlsson - 2009 - Topology and data.pdf:application/pdf},
}

@article{ghrist_barcodes_2007,
	title = {Barcodes: {The} persistent topology of data},
	volume = {45},
	issn = {0273-0979},
	shorttitle = {Barcodes},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-07-01191-3},
	doi = {10.1090/S0273-0979-07-01191-3},
	abstract = {This article surveys recent work of Carlsson and collaborators on applications of computational algebraic topology to problems of feature detection and shape recognition in high-dimensional data. The primary mathematical tool considered is a homology theory for point-cloud data sets—persistent homology—and a novel representation of this algebraic characterization—barcodes. We sketch an application of these techniques to the classiﬁcation of natural images.},
	language = {en},
	number = {01},
	urldate = {2021-11-02},
	journal = {Bulletin of the American Mathematical Society},
	author = {Ghrist, Robert},
	month = oct,
	year = {2007},
	pages = {61--76},
	file = {Ghrist - 2007 - Barcodes The persistent topology of data.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\7DXV4VPL\\Ghrist - 2007 - Barcodes The persistent topology of data.pdf:application/pdf},
}

@article{otter_roadmap_2017,
	title = {A roadmap for the computation of persistent homology},
	volume = {6},
	issn = {2193-1127},
	url = {http://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0109-5},
	doi = {10.1140/epjds/s13688-017-0109-5},
	abstract = {Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The ﬁeld of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to diﬀerent types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {EPJ Data Science},
	author = {Otter, Nina and Porter, Mason A and Tillmann, Ulrike and Grindrod, Peter and Harrington, Heather A},
	month = dec,
	year = {2017},
	pages = {17},
	file = {Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\R3DEF7P9\\Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf:application/pdf},
}

@article{zomorodian_computing_nodate,
	title = {Computing {Persistent} {Homology}},
    language = {en},
    year = {2005},
	author = {Zomorodian, Afra},
	pages = {15},
	file = {Zomorodian - Computing Persistent Homology.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\Z9DIGA7L\\Zomorodian - Computing Persistent Homology.pdf:application/pdf},
}

@inproceedings{chazal_proximity_2009,
	address = {Aarhus, Denmark},
	title = {Proximity of persistence modules and their diagrams},
	isbn = {978-1-60558-501-7},
	url = {http://portal.acm.org/citation.cfm?doid=1542362.1542407},
	doi = {10.1145/1542362.1542407},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 25th annual symposium on {Computational} geometry - {SCG} '09},
	publisher = {ACM Press},
	author = {Chazal, Frédéric and Cohen-Steiner, David and Glisse, Marc and Guibas, Leonidas J. and Oudot, Steve Y.},
	year = {2009},
	pages = {237},
	file = {Chazal et al. - 2009 - Proximity of persistence modules and their diagram.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\CE9Y5Q9T\\Chazal et al. - 2009 - Proximity of persistence modules and their diagram.pdf:application/pdf},
}

@article{phillips_notes,
	title = {Course Notes for CS 6955 Data Mining Spring 2013},
    language = {en},
	author = {Phillips, Jeff M. },
	year = {2013},
}


@article{okun_diverse_2015,
	title = {Diverse coupling of neurons to populations in sensory cortex},
	volume = {521},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature14273},
	doi = {10.1038/nature14273},
	abstract = {Exploring the relationship between population coupling and neuronal activity reveals that neighbouring neurons can differ in their coupling to the overall firing rate of the population, the circuitry of which may potentially help to explain the complex activity patterns in cortical populations.},
	number = {7553},
	journal = {Nature},
	author = {Okun, Michael and Steinmetz, Nicholas A. and Cossell, Lee and Iacaruso, M. Florencia and Ko, Ho and Barthó, Péter and Moore, Tirin and Hofer, Sonja B. and Mrsic-Flogel, Thomas D. and Carandini, Matteo and Harris, Kenneth D.},
	month = may,
	year = {2015},
	pages = {511--515},
}

@article{sadtler_neural_2014,
	title = {Neural constraints on learning},
	volume = {512},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature13665},
	doi = {10.1038/nature13665},
	abstract = {During learning, the new patterns of neural population activity that develop are constrained by the existing network structure so that certain patterns can be generated more readily than others.},
	number = {7515},
	journal = {Nature},
	author = {Sadtler, Patrick T. and Quick, Kristin M. and Golub, Matthew D. and Chase, Steven M. and Ryu, Stephen I. and Tyler-Kabara, Elizabeth C. and Yu, Byron M. and Batista, Aaron P.},
	month = aug,
	year = {2014},
	pages = {423--426},
}

@article{tsodyks_attractor_1999,
	title = {Attractor neural network models of spatial maps in hippocampus},
	volume = {9},
	issn = {1050-9631},
	url = {https://doi.org/10.1002/(SICI)1098-1063(1999)9:4<481::AID-HIPO14>3.0.CO;2-S},
	doi = {10.1002/(SICI)1098-1063(1999)9:4<481::AID-HIPO14>3.0.CO;2-S},
	abstract = {Abstract Hippocampal pyramidal neurons in rats are selectively activated at specific locations in an environment (O'Keefe and Dostrovsky, Brain Res 1971;34:171?175). Different cells are active in different places, therefore providing a faithful representation of the environment in which every spatial location is mapped to a particular population state of activity of place cells (Wilson and McNaughton, Science 1993;261:1055?1058; Zhang et al., J Neurosci 1998;79:1017?1044). We describe a theory of the hippocampus, according to which the map results from the cooperative dynamics of network, in which the strength of synaptic interaction between the neurons depends on the distance between their place fields. This synaptic structure guarantees that the network possesses a quasi-continuous set of stable states (attractors) that are localized in the space of neuronal variables reflecting their synaptic interactions, rather than their physical location in the hippocampus. As a consequence of the stable states, the network can exhibit place selective activity even without relying on input from external sensory cues. Hippocampus 1999;9:481? 489. ? 1999 Wiley-Liss, Inc.},
	number = {4},
	urldate = {2021-11-02},
	journal = {Hippocampus},
	author = {Tsodyks, Misha},
	month = jan,
	year = {1999},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {481--489},
}

@article{stopfer_intensity_2003,
	title = {Intensity versus {Identity} {Coding} in an {Olfactory} {System}},
	volume = {39},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S089662730300535X},
	doi = {10.1016/j.neuron.2003.08.011},
	abstract = {We examined the encoding and decoding of odor identity and intensity by neurons in the antennal lobe and the mushroom body, first and second relays, respectively, of the locust olfactory system. Increased odor concentration led to changes in the firing patterns of individual antennal lobe projection neurons (PNs), similar to those caused by changes in odor identity, thus potentially confounding representations for identity and concentration. However, when these time-varying responses were examined across many PNs, concentration-specific patterns clustered by identity, resolving the apparent confound. This is because PN ensemble representations changed relatively continuously over a range of concentrations of each odorant. The PNs' targets in the mushroom body—Kenyon cells (KCs)—had sparse identity-specific responses with diverse degrees of concentration invariance. The tuning of KCs to identity and concentration and the patterning of their responses are consistent with piecewise decoding of their PN inputs over oscillation-cycle length epochs.},
	number = {6},
	journal = {Neuron},
	author = {Stopfer, Mark and Jayaraman, Vivek and Laurent, Gilles},
	month = sep,
	year = {2003},
	pages = {991--1004},
}

@article{yu_gaussian-process_2009,
	title = {Gaussian-{Process} {Factor} {Analysis} for {Low}-{Dimensional} {Single}-{Trial} {Analysis} of {Neural} {Population} {Activity}},
	volume = {102},
	issn = {0022-3077},
	url = {https://doi.org/10.1152/jn.90941.2008},
	doi = {10.1152/jn.90941.2008},
	abstract = {We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from many neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional, noisy spiking activity in a compact form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the spike trains are first smoothed over time, then a static dimensionality-reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way and that account for spiking variability, which may vary both across neurons and across time. We then present a novel method for extracting neural trajectories?Gaussian-process factor analysis (GPFA)?which unifies the smoothing and dimensionality-reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that the proposed extensions improved the predictive ability of the two-stage methods. The predictive ability was further improved by going to GPFA. From the extracted trajectories, we directly observed a convergence in neural state during motor planning, an effect that was shown indirectly by previous studies. We then show how such methods can be a powerful tool for relating the spiking activity across a neural population to the subject's behavior on a single-trial basis. Finally, to assess how well the proposed methods characterize neural population activity when the underlying time course is known, we performed simulations that revealed that GPFA performed tens of percent better than the best two-stage method.},
	number = {1},
	urldate = {2021-11-02},
	journal = {Journal of Neurophysiology},
	author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
	month = jul,
	year = {2009},
	note = {Publisher: American Physiological Society},
	pages = {614--635},
}
@article{eastman_pydiffmap_2017, 
title={OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}, volume={13}, DOI={10.1371/journal.pcbi.1005659}, number={7}, journal={PLOS Computational Biology}, author={Eastman, Peter and Swails, Jason and Chodera, John D. and Mcgibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and et al.}, year={2017}}

@article{ctralie2018ripser,
    doi = {10.21105/joss.00925},
    url = {https://doi.org/10.21105/joss.00925},
    year  = {2018},
    month = {Sep},
    publisher = {The Open Journal},
    volume = {3},
    number = {29},
    pages = {925},
    author = {Christopher Tralie and Nathaniel Saul and Rann Bar-On},
    title = {{Ripser.py}: A Lean Persistent Homology Library for Python},
    journal = {The Journal of Open Source Software}
}

@book{gudhi:urm
, title        = "{GUDHI} User and Reference Manual"
, author      = "{The GUDHI Project}"
, publisher     = "{GUDHI Editorial Board}"
, edition =     "{3.4.1}"
, year         = 2021
, url =    "https://gudhi.inria.fr/python/latest/wasserstein_distance_user.html"
}


@article{mathews_molecular_2019,
	title = {Molecular phenotyping using networks, diffusion, and topology: soft tissue sarcoma},
	volume = {9},
	issn = {2045-2322},
	shorttitle = {Molecular phenotyping using networks, diffusion, and topology},
	url = {http://www.nature.com/articles/s41598-019-50300-2},
	doi = {10.1038/s41598-019-50300-2},
	abstract = {Abstract
            
              Many biological datasets are high-dimensional yet manifest an underlying order. In this paper, we describe an unsupervised data analysis methodology that operates in the setting of a multivariate dataset and a network which expresses influence between the variables of the given set. The technique involves network geometry employing the Wasserstein distance, global spectral analysis in the form of diffusion maps, and topological data analysis using the Mapper algorithm. The prototypical application is to gene expression profiles obtained from RNA-Seq experiments on a collection of tissue samples, considering only genes whose protein products participate in a known pathway or network of interest. Employing the technique, we discern several coherent states or signatures displayed by the gene expression profiles of the sarcomas in the Cancer Genome Atlas along the TP53 (p53) signaling network. The signatures substantially recover the leiomyosarcoma, dedifferentiated liposarcoma (DDLPS), and synovial sarcoma histological subtype diagnoses, and they also include a new signature defined by activation and inactivation of about a dozen genes, including activation of serine endopeptidase inhibitor
              SERPINE1
              and inactivation of TP53-family tumor suppressor gene
              TP73
              .},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Scientific Reports},
	author = {Mathews, James C. and Pouryahya, Maryam and Moosmüller, Caroline and Kevrekidis, Yannis G. and Deasy, Joseph O. and Tannenbaum, Allen},
	month = dec,
	year = {2019},
	pages = {13982},
	file = {Mathews et al. - 2019 - Molecular phenotyping using networks, diffusion, a.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\BEDW6WQA\\Mathews et al. - 2019 - Molecular phenotyping using networks, diffusion, a.pdf:application/pdf},
}

@article{naitzat_topology_nodate,
	title = {Topology of {Deep} {Neural} {Networks}},
	abstract = {We study how the topology of a data set M = Ma ∪Mb ⊆ Rd, representing two classes a and b in a binary classiﬁcation problem, changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and near-zero generalization error (≈ 0.01\%). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrarily well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following:  Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of M we begin with, when passed through a well-trained neural network f : Rd → Rp, there is a vast reduction in the Betti numbers of both components Ma and Mb; in fact they nearly always reduce to their lowest possible values: βk f (Mi) = 0 for k ≥ 1 and β0 f (Mi) = 1, i = a, b.  The reduction in Betti numbers is signiﬁcantly faster for ReLU activation than for hyperbolic tangent activation as the former deﬁnes nonhomeomorphic maps that change topology, whereas the latter deﬁnes homeomorphic maps that preserve topology.  Shallow and deep networks transform data sets diﬀerently — a shallow network operates mainly through changing geometry and changes topology only in its ﬁnal layers, a deep one spreads topological changes more evenly across all layers.},
	language = {en},
	author = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
	pages = {40},
	file = {Naitzat et al. - Topology of Deep Neural Networks.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\VR5BTJ9G\\Naitzat et al. - Topology of Deep Neural Networks.pdf:application/pdf},
}


@article{chaudhuri_intrinsic_2019,
	title = {The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep},
	volume = {22},
	issn = {1546-1726},
	url = {https://doi.org/10.1038/s41593-019-0460-x},
	doi = {10.1038/s41593-019-0460-x},
	abstract = {Neural circuits construct distributed representations of key variables—external stimuli or internal constructs of quantities relevant for survival, such as an estimate of one’s location in the world—as vectors of population activity. Although population activity vectors may have thousands of entries (dimensions), we consider that they trace out a low-dimensional manifold whose dimension and topology match the represented variable. This manifold perspective enables blind discovery and decoding of the represented variable using only neural population activity (without knowledge of the input, output, behavior or topography). We characterize and directly visualize manifold structure in the mammalian head direction circuit, revealing that the states form a topologically nontrivial one-dimensional ring. The ring exhibits isometry and is invariant across waking and rapid eye movement sleep. This result directly demonstrates that there are continuous attractor dynamics and enables powerful inference about mechanism. Finally, external rather than internal noise limits memory fidelity, and the manifold approach reveals new dynamical trajectories during sleep.},
	number = {9},
	journal = {Nature Neuroscience},
	author = {Chaudhuri, Rishidev and Gerçek, Berk and Pandey, Biraj and Peyrache, Adrien and Fiete, Ila},
	month = sep,
	year = {2019},
	pages = {1512--1520},
}
@article{singh_top_v1_2008,
    author = {Singh, Gurjeet and Memoli, Facundo and Ishkhanov, Tigran and Sapiro, Guillermo and Carlsson, Gunnar and Ringach, Dario L.},
    title = "{Topological analysis of population activity 
in visual cortex}",
    journal = {Journal of Vision},
    volume = {8},
    number = {8},
    pages = {11-11},
    year = {2008},
    month = {06},
    abstract = "{ Information in the cortex is thought to be represented by the joint activity of neurons. Here we describe how fundamental questions about neural representation can be cast in terms of the topological structure of population activity. A new method, based on the concept of persistent homology, is introduced and applied to the study of population activity in primary visual cortex (V1). We found that the topological structure of activity patterns when the cortex is spontaneously active is similar to those evoked by natural image stimulation and consistent with the topology of a two sphere. We discuss how this structure could emerge from the functional organization of orientation and spatial frequency maps and their mutual relationship. Our findings extend prior results on the relationship between spontaneous and evoked activity in V1 and illustrates how computational topology can help tackle elementary questions about the representation of information in the nervous system.}",
    issn = {1534-7362},
    doi = {10.1167/8.8.11},
    url = {https://doi.org/10.1167/8.8.11},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/933530/jov-8-8-11.pdf},
}


@article{Blumenfeld_2006,
author = {Blumenfeld, Phyllis and Rogat, Toni and Krajcik, Joseph},
year = {2006},
month = {01},
pages = {},
title = {Motivation and Cognitive Engagement in Learning Environments.},
journal = {The Cambridge Handbook of the Learning Sciences}
}


@article{ben-yishai_theory_1995,
	title = {Theory of orientation tuning in visual cortex.},
	volume = {92},
	url = {http://www.pnas.org/content/92/9/3844.abstract},
	doi = {10.1073/pnas.92.9.3844},
	abstract = {The role of intrinsic cortical connections in processing sensory input and in generating behavioral output is poorly understood. We have examined this issue in the context of the tuning of neuronal responses in cortex to the orientation of a visual stimulus. We analytically study a simple network model that incorporates both orientation-selective input from the lateral geniculate nucleus and orientation-specific cortical interactions. Depending on the model parameters, the network exhibits orientation selectivity that originates from within the cortex, by a symmetry-breaking mechanism. In this case, the width of the orientation tuning can be sharp even if the lateral geniculate nucleus inputs are only weakly anisotropic. By using our model, several experimental consequences of this cortical mechanism of orientation tuning are derived. The tuning width is relatively independent of the contrast and angular anisotropy of the visual stimulus. The transient population response to changing of the stimulus orientation exhibits a slow "virtual rotation." Neuronal cross-correlations exhibit long time tails, the sign of which depends on the preferred orientations of the cells and the stimulus orientation.},
	number = {9},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ben-Yishai, R and Bar-Or, R L and Sompolinsky, H},
	month = apr,
	year = {1995},
	pages = {3844},
}

@article{goldberg_randomized_2004,
	title = {A {Randomized} {Controlled} {Trial} of {Fluorouracil} {Plus} {Leucovorin}, {Irinotecan}, and {Oxaliplatin} {Combinations} in {Patients} {With} {Previously} {Untreated} {Metastatic} {Colorectal} {Cancer}},
	volume = {22},
	issn = {0732-183X, 1527-7755},
	url = {http://ascopubs.org/doi/10.1200/JCO.2004.09.046},
	doi = {10.1200/JCO.2004.09.046},
	abstract = {Purpose Three agents with differing mechanisms of action are available for treatment of advanced colorectal cancer: ﬂuorouracil, irinotecan, and oxaliplatin. In this study, we compared the activity and toxicity of three different two-drug combinations in patients with metastatic colorectal cancer who had not been treated previously for advanced disease. Patients and Methods Patients were concurrently randomly assigned to receive irinotecan and bolus ﬂuorouracil plus leucovorin (IFL, control combination), oxaliplatin and infused ﬂuorouracil plus leucovorin (FOLFOX), or irinotecan and oxaliplatin (IROX). The primary end point was time to progression, with secondary end points of response rate, survival time, and toxicity.
Results A total of 795 patients were randomly assigned between May 1999 and April 2001. A median time to progression of 8.7 months, response rate of 45\%, and median survival time of 19.5 months were observed for FOLFOX. These results were signiﬁcantly superior to those observed for IFL for all end points (6.9 months, 31\%, and 15.0 months, respectively) or for IROX (6.5 months, 35\%, and 17.4 months, respectively) for time to progression and response. The FOLFOX regimen had signiﬁcantly lower rates of severe nausea, vomiting, diarrhea, febrile neutropenia, and dehydration. Sensory neuropathy and neutropenia were more common with the regimens containing oxaliplatin.
Conclusion The FOLFOX regimen of oxaliplatin and infused ﬂuorouracil plus leucovorin was active and comparatively safe. It should be considered as a standard therapy for patients with advanced colorectal cancer.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Journal of Clinical Oncology},
	author = {Goldberg, Richard M. and Sargent, Daniel J. and Morton, Roscoe F. and Fuchs, Charles S. and Ramanathan, Ramesh K. and Williamson, Stephen K. and Findlay, Brian P. and Pitot, Henry C. and Alberts, Steven R.},
	month = jan,
	year = {2004},
	pages = {23--30},
	file = {Goldberg et al. - 2004 - A Randomized Controlled Trial of Fluorouracil Plus.pdf:C\:\\Users\\ifisa\\Zotero\\storage\\9GTH4AQD\\Goldberg et al. - 2004 - A Randomized Controlled Trial of Fluorouracil Plus.pdf:application/pdf},
}

@article{edelsbrunner_topological_2002,
	title = {Topological {Persistence} and {Simplification}},
	volume = {28},
	issn = {1432-0444},
	url = {https://doi.org/10.1007/s00454-002-2885-2},
	doi = {10.1007/s00454-002-2885-2},
	abstract = {We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise depending on its lifetime or persistence within the filtration. We give fast algorithms for computing persistence and experimental evidence for their speed and utility.},
	number = {4},
	journal = {Discrete \& Computational Geometry},
	author = {{Edelsbrunner} and {Letscher} and {Zomorodian}},
	month = nov,
	year = {2002},
	pages = {511--533},
}
@TECHREPORT{Krizhevsky09cifar-10,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}
@software{tensortoolbox,
  author = {Brett W. Bader, Tamara G. Kolda and others},
  title = {Tensor Toolbox for MATLAB},
  url = {https://www.tensortoolbox.org/},
  version = {3.2.1},
  date = {2021-04-05},
}
@ARTICLE{mnist,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
  
@phdthesis{stanley_geometric_2020,
	address = {Ann Arbor},
	type = {Ph.{D}.},
	title = {Geometric {Data} {Organization}: {Algorithms} and {Applications}},
	url = {https://www.proquest.com/dissertations-theses/geometric-data-organization-algorithms/docview/2544526068/se-2?accountid=10267},
	abstract = {High dimensional and high throughput technologies are central tools of modern scientific research. These tools cast a wide net with the goal of amassing sufficiently strong signals to detect subtle phenomena in complex systems. Yet, the complexity and size of modern data is precisely the crux of contemporary data science. Without processing, high dimensional data is indecipherable to a human; thus, a central goal of machine learning is to reduce massive datasets into interpretable, rich representations. Secondly, the immense scale of modern data poses a computational challenge that demands the development of efficient algorithms. In this thesis, we leverage the intrinsic geometry of data to simultaneously address challenges of high dimensionality and large sample datasets, developing representation learning algorithms that efficiently scale for large datasets. Towards the former, we construct algorithms for data exploration by augmenting data according to its geometry and characterizing the geometrically distribution of data points from different experimental conditions in the representation space. For the latter, we use data geometry to develop algorithms for learning compressed data representations that can be computed quickly as well as unified representations of large multi-experiment and multi-modality datasets. To demonstrate our approaches, we tackle general machine learning problems including classification, clustering, and regression. As an application domain, we particularly focus on single cell RNA sequencing (scRNA-seq) and also consider applications to single cell chromatin profiling (scATAC-seq) and mass cytometry (CyTOF).},
	language = {English},
	school = {Yale University},
	author = {Stanley, III, Jay S.},
	year = {2020},
	note = {ISBN: 9798516931055
Publication Title: ProQuest Dissertations and Theses
27736931},
	keywords = {0984:Computer science, Algorithms, Computer science, 0364:Applied Mathematics, 0715:Bioinformatics, Applied mathematics, Big data, Bioinformatics, Data geometry, Dimensionality reduction, Manifold learning, Single cell biology},
}


@misc{feature_map,
  title = {How to Visualize Filters and Feature Maps in Convolutional Neural Networks},
  howpublished = {\url{https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/}},
  note = {Accessed: 2021-11-08}
}

@misc{cs231n,
  title = {Notes for CS231n: Convolutional Neural Networks for Visual Recognition.},
  howpublished = {\url{https://cs231n.github.io/}},
  note = {Accessed: 2021-11-08}
}